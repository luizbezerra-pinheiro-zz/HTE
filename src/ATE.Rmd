---
title: "R Notebook"
output: html_notebook
---

```{r}
# Load the libraries

library("Amelia")
library("MatchIt")
library("Zelig")
library("sandwich")
library("twang") # library for ps() function
library("cluster")
library("rlist") # Append
library("dplyr")
library("mice")
library("FactoMineR")
library("factoextra")
```


Load pre-treated data
```{r}
# Clear any existing variables
rm(list = ls())

# Set seed for reproducibility
set.seed(1)

dir_HTE = "~/Documents/Projects/HTE/"
dir_data = "data/data_preprocessed_tbi_individuals.csv"

# Load pretreated_dataset
pretreated_dataset <- as.data.frame(read.csv(paste(dir_HTE, dir_data, sep = ""), header = TRUE))
pretreated_dataset <- pretreated_dataset %>% plyr::rename(c(Tranexamic.acid = "W", Death = "Y"))
levels(pretreated_dataset$Cardiac.arrest.ph) <- c(0,1)
pretreated_dataset$Cardiac.arrest.ph <- as.integer(pretreated_dataset$Cardiac.arrest.ph) - 1

# Overrite pre-treated Dataset with columns renamed 
pretreated_dataset
```

Initial Study on pretreateddata
```{r}
# Glasgow Coma Scale (GCS.init)

glasgow_stats <- function (dataset){
  T_treated <- dataset %>% filter(W == 1) %>% nrow(.)
  T_control <- dataset %>% filter(W == 0) %>% nrow(.)
  glasgow_severe <- dataset %>% filter(GCS.init <= 8, GCS.init >= 3)
  glasgow_moderate <- dataset %>% filter(GCS.init <= 12, GCS.init >= 9)
  glasgow_mild <- dataset %>% filter(GCS.init <= 15, GCS.init >= 13)
  glasgow_unknown <- dataset %>% filter(is.na(GCS.init))
  
  gw <- list(glasgow_severe, glasgow_moderate, glasgow_mild, glasgow_unknown)
  res <- c()
  for (i in gw){
    n_treated = i %>% filter(W == 1) %>% nrow(.)
    n_control = i %>% filter(W == 0) %>% nrow(.)
    stats <- c(n_treated = n_treated, "%_treated" = round(100 * n_treated/T_treated, digits = 2), n_control = n_control, "%_control" = round(100*n_control/T_control, digits = 2))
    res <- rbind(res, stats)
  }
  rownames(res) <- c("Severe (3-8)", "Moderate (9-12)", "Mild (13-15)", "Unknown")
  return(res)
}

stats_glasgow <- glasgow_stats(pretreated_dataset)
stats_glasgow

# Pupil Anomaly (Pupil.anomaly)

pupil_stats <- function (dataset){
  T_treated <- dataset %>% filter(W == 1) %>% nrow(.)
  T_control <- dataset %>% filter(W == 0) %>% nrow(.)
  
  pupil_none <- dataset %>% filter(Pupil.anomaly == "Non")
  pupil_one <- dataset %>% filter(Pupil.anomaly == "Anisocorie (unilatérale)")
  pupil_both <- dataset %>% filter(Pupil.anomaly == "Mydriase (bilatérale)")
  pupil_unknown <- dataset %>% filter(is.na(Pupil.anomaly))
  
  pa <- list(pupil_none, pupil_one, pupil_both, pupil_unknown)
  res <- c()
  for (i in pa){
    n_treated = i %>% filter(W == 1) %>% nrow(.)
    n_control = i %>% filter(W == 0) %>% nrow(.)
    stats <- c(n_treated = n_treated, "%_treated" = round(100 * n_treated/T_treated, digits = 2) , n_control = n_control, "%_control" = round(100*n_control/T_control, digits = 2))
    res <- rbind(res, stats)
  }
  rownames(res) <- c("Both reacts", "One reacts", "None reacts",  "Unknown")
  return(res)
}

stats_pupil <- pupil_stats(pretreated_dataset)
stats_pupil


# Head injury death by severity

death_by_severity_gcs_stats <- function(dataset){
  # GCS mild to moderate (9-15)
  n_treated = dataset %>% filter(GCS.init <= 15, GCS.init >= 9, W == 1) %>% nrow()
  n_treated_dead = dataset %>% filter(GCS.init <= 15, GCS.init >= 9, W==1, Y == 1) %>% nrow()
  perc_treated_dead =  100*(n_treated_dead / n_treated) %>% round(digits = 4)
  
  n_control = dataset %>% filter(GCS.init <= 15, GCS.init >= 9, W == 0) %>% nrow()
  n_control_dead = dataset %>% filter(GCS.init <= 15, GCS.init >= 9, W == 0, Y == 1) %>% nrow()
  perc_control_dead = 100*(n_control_dead / n_control) %>% round(digits = 4)
  
  gcs_m <- c(Treated = paste(n_treated_dead, "/", n_treated, " (", perc_treated_dead, "%)", sep = ""), Control = paste(n_control_dead, "/", n_control, " (", perc_control_dead, "%)", sep = ""))
  
  # GCS severe (3-8)
  n_treated = dataset %>% filter(GCS.init <= 8, GCS.init >= 3, W == 1) %>% nrow()
  n_treated_dead = dataset %>% filter(GCS.init <= 8, GCS.init >= 3, W==1, Y == 1) %>% nrow() 
  perc_treated_dead =  100*(n_treated_dead / n_treated) %>% round(digits = 4)
  
  n_control = dataset %>% filter(GCS.init <= 8, GCS.init >= 3, W == 0) %>% nrow()
  n_control_dead = dataset %>% filter(GCS.init <= 8, GCS.init >= 3, W == 0, Y == 1) %>% nrow()
  perc_control_dead = 100*(n_control_dead / n_control) %>% round(digits = 4)
  
  gcs_s <- c(Treated = paste(n_treated_dead, "/", n_treated, " (", perc_treated_dead, "%)", sep = ""), Control = paste(n_control_dead, "/", n_control, " (", perc_control_dead, "%)", sep = ""))
  
  # Overrall
  n_treated = dataset %>% filter(W == 1) %>% nrow()
  n_treated_dead = dataset %>% filter(W==1, Y == 1) %>% nrow()
  perc_treated_dead = 100*(n_treated_dead / n_treated) %>% round(digits = 4)
  
  n_control = dataset %>% filter(W == 0) %>% nrow()
  n_control_dead = dataset %>% filter(W == 0, Y == 1) %>% nrow()
  perc_control_dead = 100*(n_control_dead / n_control) %>% round(digits = 4)
  
  gcs_o <- c(Treated = paste(n_treated_dead, "/", n_treated, " (", perc_treated_dead, "%)", sep = ""), Control = paste(n_control_dead, "/", n_control, " (", perc_control_dead, "%)", sep = ""))
  res <- rbind(gcs_m, gcs_s, gcs_o)
  row.names(res) <- c("Mild to moderate (9-15)", "Severe (3-8)", "Overrall")
  return(res)
}

death_by_severity_pupil_stats <- function(dataset){
  # Pupil.anomaly Both reacts
  n_treated = dataset %>% filter(Pupil.anomaly == "Non", W == 1) %>% nrow()
  n_treated_dead = dataset %>% filter(Pupil.anomaly == "Non", W==1, Y == 1) %>% nrow()
  perc_treated_dead =  100*(n_treated_dead / n_treated) %>% round(digits = 4)
  
  n_control = dataset %>% filter(Pupil.anomaly == "Non", W == 0) %>% nrow()
  n_control_dead = dataset %>% filter(Pupil.anomaly == "Non", W == 0, Y == 1) %>% nrow()
  perc_control_dead = 100*(n_control_dead / n_control) %>% round(digits = 4)
  
  pupil_m <- c(Treated = paste(n_treated_dead, "/", n_treated, " (", perc_treated_dead, "%)", sep = ""), Control = paste(n_control_dead, "/", n_control, " (", perc_control_dead, "%)", sep = ""))
  
  # Pupil Anomaly any non reactive
  n_treated = dataset %>% filter(Pupil.anomaly != "Non", !is.na(Pupil.anomaly), W == 1) %>% nrow()
  n_treated_dead = dataset %>% filter(Pupil.anomaly != "Non", !is.na(Pupil.anomaly), W==1, Y == 1) %>% nrow() 
  perc_treated_dead =  100*(n_treated_dead / n_treated) %>% round(digits = 4)
  
  n_control = dataset %>% filter(Pupil.anomaly != "Non", !is.na(Pupil.anomaly), W == 0) %>% nrow()
  n_control_dead = dataset %>% filter(Pupil.anomaly != "Non", !is.na(Pupil.anomaly), W == 0, Y == 1) %>% nrow()
  perc_control_dead = 100*(n_control_dead / n_control) %>% round(digits = 4)
  
  pupil_s <- c(Treated = paste(n_treated_dead, "/", n_treated, " (", perc_treated_dead, "%)", sep = ""), Control = paste(n_control_dead, "/", n_control, " (", perc_control_dead, "%)", sep = ""))
  
  # Overrall
  n_treated = dataset %>% filter(W == 1) %>% nrow()
  n_treated_dead = dataset %>% filter(W==1, Y == 1) %>% nrow()
  perc_treated_dead = 100*(n_treated_dead / n_treated) %>% round(digits = 4)
  
  n_control = dataset %>% filter(W == 0) %>% nrow()
  n_control_dead = dataset %>% filter(W == 0, Y == 1) %>% nrow()
  perc_control_dead = 100*(n_control_dead / n_control) %>% round(digits = 4)
  
  pupil_o <- c(Treated = paste(n_treated_dead, "/", n_treated, " (", perc_treated_dead, "%)", sep = ""), Control = paste(n_control_dead, "/", n_control, " (", perc_control_dead, "%)", sep = ""))
  res <- rbind(pupil_m, pupil_s, pupil_o)
  row.names(res) <- c("Both reactive", "Any non reactive", "Overrall")
  return(res)
}


death_by_severity_gcs_stats(pretreated_dataset)
death_by_severity_pupil_stats(pretreated_dataset)


write.csv(stats_glasgow, paste(dir_HTE, "data/stats_glasgow.csv", sep = ""))
write.csv(stats_pupil, paste(dir_HTE, "data/stats_pupil.csv", sep = ""))
write.csv(death_by_severity_gcs_stats(pretreated_dataset), paste(dir_HTE, "data/death_glasgow.csv", sep = ""))
write.csv(death_by_severity_pupil_stats(pretreated_dataset), paste(dir_HTE, "data/death_pupil.csv", sep = ""))

```

Filling Missing Values with MICE
```{r}

completeData <- function(dataset, idvars, m_){
  init = mice(dataset, maxit=0) 
  predM = init$predictorMatrix
  predM[, idvars] <- 0
  tempData <- mice(dataset, predictorMatrix=predM, m=m_,maxit=10,meth= "pmm",seed=500)

  df <- complete(tempData, 1) %>% as.data.frame(.)

  return (df)
}

#Some variables of our dataset are only control variables, it means they shouldn't be used for predictions, as for example the pacient id(X)
idvars <- c("X")
m_ <- 1

completed_dataframe <- completeData(pretreated_dataset, idvars = idvars, m_)

completed_dataframe
```

Variables Selection - Select only the pre-treatment variables (Red on figure) for each dataframe + GCS.init and Pupil.Anomaly

```{r}
# Selecting the variables used to determine the if the person is getting or not the treatment 
cts_variables_names <- c("SBP.ph", "DBP.ph", "HR.ph", "Shock.index.ph", "Delta.shock.index", "Cristalloid.volume", "Colloid.volume", "HemoCue.init", "Delta.hemoCue", "SpO2.ph.min")
binary_variables_names <- c("Vasopressor.therapy", "AIS.external", "Cardiac.arrest.ph")
severity_variables_names <- c("GCS.init", "Pupil.anomaly")

# categorical_variables_names <- c("Trauma.center") # We will ignore this one
covariates <- c(cts_variables_names, binary_variables_names)

all_variables_names <- c(covariates, severity_variables_names, "W", "Y")

reduced_dataframe <- completed_dataframe %>% select("X", all_variables_names) %>% as.data.frame()

reduced_dataframe

```





Clustering By Severity
```{r}
cluster_by_severity <- function(dataset){
  # Returns 6 datasets:
  # 1 - GCS mild to moderate (8-15)
  # 2 - GCS severe (3-7)
  # 3 - GCS mild to moderate (8-15) and Pupil.Anomaly aggravating (Any non reactive)
  # 4 - GCS mild to moderate (8-15) and Pupil.Anomaly non-aggravating (both reactive)
  # 5 - GCS mild to severe (3-7) and Pupil.Anomaly aggravating (Any non reactive)
  # 6 - GCS mild to severe (3-7) and Pupil.Anomaly non-aggravating (both reactive)
  
  # Divise by GCS
  dataset_GCS_moderate <- dataset %>% filter(GCS.init <= 15, GCS.init >= 9)
  dataset_GCS_severe <- dataset %>% filter(GCS.init <= 8, GCS.init >= 3)
  
  # Subdivise by pupil anomaly aggravation
  dataset_GCS_moderate_agg <- dataset_GCS_moderate %>% filter(Pupil.anomaly != "Non") %>% select(-severity_variables_names)
  dataset_GCS_moderate_no_agg <- dataset_GCS_moderate %>% filter(Pupil.anomaly == "Non") %>% select(-severity_variables_names)
  
  dataset_GCS_severe_agg <- dataset_GCS_severe %>% filter(Pupil.anomaly != "Non") %>% select(-severity_variables_names)
  dataset_GCS_severe_no_agg <- dataset_GCS_severe %>% filter(Pupil.anomaly == "Non") %>% select(-severity_variables_names)
  
  
  # Remove severity columns
  dataset_GCS_moderate <- dataset_GCS_moderate %>% select(-severity_variables_names)
  dataset_GCS_severe <- dataset_GCS_severe %>% select(-severity_variables_names)
  
  return(list(dataset_GCS_moderate, dataset_GCS_severe, dataset_GCS_moderate_agg, dataset_GCS_moderate_no_agg, dataset_GCS_severe_agg, dataset_GCS_severe_no_agg))
}

dataframes_clustered <- cluster_by_severity(reduced_dataframe)
dataframes_clustered

```



Propensity score calculation for each cluster 
```{r}
# Computing the propensity socre by logistic regression of W on X
logistic_ps <- function(dataset, psFormula){
  p_logistic.fit <- glm(psFormula, data = dataset, family = "binomial")
  p_logistic <- predict(p_logistic.fit, type = "response")
  hist(p_logistic)
  return (p_logistic)
}

#Estimate propensity scores with generalized boosted modeling (GBM)
gbm_ps <- function(dataset, psFormula){
  # es: refers to standardized ef-fect size.
  myGBM <- ps(psFormula, data = dataset, n.trees=10000, interaction.depth=4,
              shrinkage=0.01, stop.method=c("es.max"), estimand = "ATT",
              verbose=TRUE)

  #extract estimated propensity scores from object
  gbm_estimations <- myGBM$ps[, 1]
  # notice here we do not need to
  return(gbm_estimations)
}

# Let's define the formula used to calculate the propensity score WE CHOOSE THE GBM PROPENSITY SCORE TO ESTIMATE IT
psFormula = as.formula(paste("W ~", paste(covariates, collapse = " + ")))
for (i in 1:length(dataframes_clustered)){
  dataframes_clustered[[i]]$p.logistic <- logistic_ps(dataframes_clustered[[i]], psFormula)
}

# Covert all dataset to numeric
for (i in 1:length(dataframes_clustered)){
   dataframes_clustered[[i]][, binary_variables_names] <-  dataframes_clustered[[i]][, binary_variables_names] %>% sapply(as.numeric) %>% as.data.frame(.)  
}


dataframes_clustered

# Compute all propensity score GBM for the reduced_dataframe to be used also on HTE
reduced_dataframe$p.logistic <- logistic_ps(reduced_dataframe, psFormula)
write.csv(reduced_dataframe, file=paste(dir_HTE, "data/reduced_dataset.csv", sep = ""))





```


ATE estimations 
```{r}
# We first define the functions

# Direct conditional mean estimation
# Let's define the conditional average treatment effect (CATE)
ate_condmean_ols <- function(dataset) {
  df_centered = data.frame(scale(dataset[, !names(dataset) %in% c("X", "p.logistic")], center = TRUE, scale = FALSE))
  # Running OLS with full interactions is like running OLS separately on
  # the treated and controls. If the design matrix has been pre-centered,
  # then the W-coefficient corresponds to the ATE.
  lm.interact = lm(Y ~ . * W, data = df_centered)
  tau.hat = as.numeric(coef(lm.interact)["W"])
  se.hat = as.numeric(sqrt(vcovHC(lm.interact, type = "HC")["W", "W"]))
  return ( c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat) )
}

# Now let's estimate the ATE using Inverse-propensity score weighting
ipw <- function(dataset, p) {
  W <- dataset$W
  Y <- dataset$Y
  G <- ((W - p) * Y) / (p * (1-p))
  tau.hat <- mean(G)
  se.hat <- sqrt(var(G) / (length(G) - 1))
  return ( c(ATE = tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.68 * se.hat) )
}

# Now Weighted OLS on W
prop_score_ols <- function(dataset, p) {
  # Pulling relevant columns
  W <- dataset$W
  Y <- dataset$Y
  # Computing weights
  weights <- (W / p) + ((1 - W) / (1 - p))
  # OLS
  lm.fit <- lm(Y ~ W, data = dataset[, !names(dataset) %in% c("X", "p.logistic")], weights = weights)
  tau.hat = as.numeric(coef(lm.fit)["W"])
  se.hat = as.numeric(sqrt(vcovHC(lm.fit, type = "HC")["W", "W"]))
  return ( c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat) )
}

# Now the double robust

aipw_ols <- function(dataset, p) {

  ols.fit = lm(Y ~ W * ., data = dataset[, !names(dataset) %in% c("X", "p.logistic", "Trauma.center")])

  dataset.treatall = dataset
  dataset.treatall$W = 1
  treated_pred = predict(ols.fit, dataset.treatall)

  dataset.treatnone = dataset
  dataset.treatnone$W = 0
  control_pred = predict(ols.fit, dataset.treatnone)

  actual_pred = predict(ols.fit, dataset)

  G <- treated_pred - control_pred +
    ((dataset$W - p) * (dataset$Y - actual_pred)) / (p * (1 - p))
  tau.hat <- mean(G)
  se.hat <- sqrt(var(G) / (length(G) - 1))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}
```

```{r}
# Now we estimate the ATE for each cluster using multiple methods
aipw_estimations <- c()
for (df in dataframes_clustered) {
  n_treated = df %>% filter(W == 1) %>% nrow()
  n_control = df %>% filter(W == 0) %>% nrow()
  mean_p.logistic = mean(df[, "p.logistic"])
  aipw_estimations <- rbind(aipw_estimations, c(n_treated = n_treated, n_control = n_control, p_mean = mean_p.logistic, aipw_ols(df, df$p.logistic)))
}

rownames(aipw_estimations) <- c("GCS Mild/Moderate (8-15)", "GCS Severe (3-7)", "GCS Mild/Moderate + Anomaly", "GCS Mild/Moderate + no Anomaly", "GCS Severe + Anomaly", "GCS Severe + no Anomaly")

aipw_estimations
```
Matching to try to reduce cofounding
```{r}
# Now let's do a matching in each dataset to try to reduce confounding
# As we have just too little patients in the smaller clusters, we will only use 2 clusters using p.logistic as propensity score
# So we do the Greedy matching, discarding Control patients, and then we re-cluster by severity

reduced_dataframe

# Greedy <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
matched_dataframe <- matchit(formula = psFormula, data = reduced_dataframe, method = "nearest", discard = "control", distance = "logit", reestimate = TRUE) %>% match.data() %>% as.data.frame()

matched_dataframe

# Now we remove the new columns automatically added by matchit
matched_dataframe$distance <- NULL
matched_dataframe$weights<- NULL
matched_dataframe$p.logistic <- logistic_ps(matched_dataframe, psFormula)

# Now we re-cluster by severity
matched_clustered_severity_dataframes <- cluster_by_severity(matched_dataframe)
matched_clustered_severity_dataframes


hist(matched_clustered_severity_dataframes[[2]]$p.logistic)
# Now we re-estimate the ATE for each cluster using AIPW method
aipw_estimations_matched <- c()
for (df in matched_clustered_severity_dataframes) {
  n_treated = df %>% filter(W == 1) %>% nrow()
  n_control = df %>% filter(W == 0) %>% nrow()
  mean_p.logistic = mean(df[, "p.logistic"])
  aipw_estimations_matched <- rbind(aipw_estimations_matched, c(n_treated = n_treated, n_control = n_control, p_mean = mean_p.logistic, aipw_ols(df, df$p.logistic)))
}

rownames(aipw_estimations_matched) <- c("GCS Mild/Moderate (8-15)", "GCS Severe (3-7)", "GCS Mild/Moderate + Anomaly", "GCS Mild/Moderate + no Anomaly", "GCS Severe + Anomaly", "GCS Severe + no Anomaly")
```

```{r}
aipw_estimations
aipw_estimations_matched
```

```{r}
# Now let's print the results
output_GCS <- cbind(aipw_estimations, aipw_estimations_matched)
print(cbind(aipw_estimations, aipw_estimations_matched))

write.csv(output_GCS, paste(dir_HTE, "data/GCS_output.csv", sep = ""))
```

Another way of clustering the dataset is using PAM clustering (try to cluster by the covariates) or Hierarchical Clustering

1) PAM clustering

```{r}

# Clustering via PAM - 2 Clusters choosed via Sillhouette

reduced_dataframe$Pupil.anomaly <- NULL
reduced_dataframe$GCS.init <- NULL

pam_clustering <- function(dataset, n_clusters){
  # Receives the full dataset from which we only use the covariates and the algorithm infers the distance using Gower Distance
  # Change the type of the variables to categorical
  dataset[, binary_variables_names] <- as.data.frame(sapply(dataset[, binary_variables_names], as.factor))

  gower_dist <- daisy(dataset[, c(cts_variables_names, binary_variables_names)], metric = "gower") # We only use the covariates
  gower_mat <- as.matrix(gower_dist)

  # Do the clustering
  pam_fit <- pam(gower_dist, diss = TRUE, k = n_clusters)
  temp_df <- dataset %>% mutate(cluster = pam_fit$clustering)
  temp_df [, binary_variables_names] <- temp_df[, binary_variables_names] %>% sapply(as.numeric) %>% as.data.frame(.)
  return (temp_df)
}

# Let's take a look in the sillhouette to determine the n_cluster (ONLY ONE TIME BECAUSE TAKES TOO LONG)
# sil_width <- c(NA)
# for(i in 2:8){
#   pam_fit <- pam(gower_dist, diss = TRUE, k = i)
#   sil_width[i] <- pam_fit$silinfo$avg.width
# }
# plot(1:8, sil_width,
#      xlab = "Number of clusters",
#      ylab = "Silhouette Width")
# lines(1:8, sil_width)

# We chose n_cluster = 2

n_clusters <- 2
set.seed(1)
pam_clustered <- pam_clustering(reduced_dataframe, n_clusters)
pam_clustered
```

Calculating the ATE for each cluster 
```{r}
# Now we estimate the ATE for each cluster using multiple methods
aipw_estimations_pam <- c()
for (i in 1:n_clusters) {
  df <- pam_clustered %>% filter(cluster == i) %>% select(-cluster)
  n_treated = df %>% filter(W == 1) %>% nrow()
  n_control = df %>% filter(W == 0) %>% nrow()
  p_mean = mean(df$p.logistic)
  aipw_estimations_pam <- rbind(aipw_estimations_pam, c(n_treated = n_treated, n_control = n_control, p.logistic = p_mean, aipw_ols(df, df$p.logistic)))
}
rownames(aipw_estimations_pam) <- c("Cluster 1", "Cluster 2")
```

```{r}
# Now let's print the results
print(aipw_estimations_pam)
```


Let's calculate the ATE before and after a genetic matching
```{r}

# ATE Before
aipw_ATE_before <- aipw_ols(reduced_dataframe, reduced_dataframe$p.logistic)
hist(reduced_dataframe$p.logistic, main = paste("Histogram of Propensity Score"), xlab = "Propensity Score")
n_treated_before = reduced_dataframe %>% filter(W == 1) %>% nrow()
n_control_before = reduced_dataframe %>% filter(W == 0) %>% nrow()

info_aipw_before <- c(n_treated = n_treated_before, n_control = n_control_before, aipw_ATE_before)
info_aipw_before

# GREEDY Matching than ATE
matched_aipw_genetic <- matchit(psFormula, data = reduced_dataframe, method = "nearest", discard = "control", distance = reduced_dataframe$p.logistic) %>% match.data() %>% as.data.frame()

matched_aipw_genetic$distance <- NULL
matched_aipw_genetic$weights <- NULL
matched_aipw_genetic$p.logistic <- logistic_ps(matched_aipw_genetic, psFormula)
hist(matched_aipw_genetic$p.logistic, main = paste("Histogram of Propensity Score after matching"), xlab = "Propensity Score")


aipw_ATE_after <- aipw_ols(matched_aipw_genetic, matched_aipw_genetic$p.logistic)
n_treated_after = matched_aipw_genetic %>% filter(W == 1) %>% nrow()
n_control_after = matched_aipw_genetic %>% filter(W == 0) %>% nrow()

ATE_aipw_genetic <- c(n_treated = n_treated_after, n_control = n_control_after, aipw_ATE_after)
ATE_aipw_genetic


# Now let's cluster the Data  (PAM Clustering) after the matching and re-estimate the ATE for each cluster
pam_matched_clustered <- pam_clustering(matched_aipw_genetic, n_clusters)
aipw_estimations_matched_pam <- c()
for (i in 1:n_clusters) {
  df <- pam_matched_clustered %>% filter(cluster == i) %>% select(-cluster)
  n_treated = df %>% filter(W == 1) %>% nrow()
  n_control = df %>% filter(W == 0) %>% nrow()
  p_mean = mean(df$p.logistic)
  aipw_estimations_matched_pam <- rbind(aipw_estimations_matched_pam, c(n_treated = n_treated, n_control = n_control, p.logistic = p_mean, aipw_ols(df, df$p.logistic)))
}
rownames(aipw_estimations_matched_pam) <- c("Cluster 1", "Cluster 2")

print(aipw_estimations_matched_pam)

```


REMOVE THIS BEFORE SENDING (CASO NINGUÈM COMSIGA DEIXAR BONITINHO)


```{r}
# TO see the cluster we will use MCA (Multiple correspondence analysis)
str(pam_clustered)
data.FAMD_analysis <- pam_clustered %>% select(-c("X", "Y", "W"))
data.FAMD_analysis[, binary_variables_names] <- data.FAMD_analysis[, binary_variables_names] %>% lapply(as.factor)

levels(data.FAMD_analysis$Vasopressor.therapy) <- c("VT-1", "VT-2")
levels(data.FAMD_analysis$AIS.external) <- c("AIS-1", "AIS-2", "AIS-3", "AIS-4", "AIS-5")
levels(data.FAMD_analysis$Cardiac.arrest.ph) <- c("CA-0", "CA-1")

str(data.FAMD_analysis)

res.famd <- FAMD(data.FAMD_analysis, graph = FALSE)
print(res.famd)
eig.val <- get_eigenvalue(res.famd)
head(eig.val)
fviz_screeplot(res.famd, addlabels = TRUE, ylim = c(0, 45))


var <- get_famd_var(res.famd)
head(var$coord)
head(var$contrib)

# Plot of variables
fviz_famd_var(res.famd, repel = TRUE)
# Contribution to the first dimension
fviz_contrib(res.famd, "var", axes = 1)
# Contribution to the second dimension
fviz_contrib(res.famd, "var", axes = 2)

quanti.var <- get_famd_var(res.famd, "quanti.var")
head(quanti.var$coord)
fviz_famd_var(res.famd, "quanti.var", repel = TRUE, col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))


fviz_famd_var(res.famd, "quali.var", col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
             )

quali.var <- get_famd_var(res.famd, "quali.var")
head(quali.var$contrib)
fviz_famd_var(res.famd, "quali.var", col.var = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
             )
fviz_famd_ind(res.famd, habillage = "cluster", palette = c("#00AFBB", "#E7B800"), repel = TRUE)
```

