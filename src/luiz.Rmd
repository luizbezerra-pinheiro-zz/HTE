---
title: "R Notebook"
output: html_notebook
---

```{r}
# Load the libraries

library("Amelia")
library("MatchIt")
library("Zelig")
library("sandwich")
library("twang") # library for ps() function
#library("wiggleplotr")
library("dplyr")
#library("GenomicRanges")
#library("GenomicFeatures")
#library("biomaRt")
```



```{r}
# Clear any existing variables
rm(list = ls())

# Set seed for reproducibility
set.seed(1)

# Load pretreated_dataset
pretreated_dataset <- read.csv("/home/luiz/Documents/Projects/HTE/data/data_preprocessed_tbi_individuals.csv", header = TRUE)

# These are the covariates we'll use

cts_variables_names <- c("SBP.ph", "DBP.ph", "HR.ph", "Shock.index.ph", "Delta.shock.index", "Cristalloid.volume", "Colloid.volume", "HemoCue.init", "Delta.hemoCue", "SpO2.ph.min")

binary_variables_names <- c("Vasopressor.therapy", "AIS.external", "Cardiac.arrest.ph")

levels(pretreated_dataset$Cardiac.arrest.ph) <- c(0,1)

categorical_variables_names <- c("X", "Trauma.center")

covariates <- c(cts_variables_names, binary_variables_names, categorical_variables_names)

all_variables_names <- c(covariates, "Death", "Tranexamic.acid")

# Extracting and scaling continuous variables
scaled_cts_covariates <- pretreated_dataset %>%
  dplyr::select(cts_variables_names) %>%
  scale()

# Extracting  indicator variables
binary_covariates <- pretreated_dataset %>%
  dplyr::select(binary_variables_names)
categorical_covariates <- pretreated_dataset %>%
  dplyr::select(categorical_variables_names)

# Extracting outcome and treatment

outcome <- pretreated_dataset %>% dplyr::select(Death)
treatment <- pretreated_dataset %>% dplyr::select(Tranexamic.acid) 

# Setting up the data with only the variables of interest, and renaming columns
df <- data.frame(categorical_covariates, scaled_cts_covariates, binary_covariates, outcome, treatment) %>% plyr::rename(c(Tranexamic.acid = "W", Death = "Y"))
# Change type of binary variables
df[, binary_variables_names] <- sapply(df[, binary_variables_names], as.numeric)

```


Missing Values

```{r}
# Now let's complet the dataframe using Amelia
m = 1
noms = c(binary_variables_names)
idvars = c("Y", "W", categorical_variables_names)
a.out <- amelia(df, m = m, noms = noms, idvars = idvars)
dfcomp <- a.out$imputations$imp1
summary(dfcomp)

# Let's define the variables
Xcomp = dfcomp[, !names(dfcomp) %in% c("Y", "W")]
Ycomp = dfcomp$Y
Wcomp = dfcomp$W

```

Propensity Score Calculation

```{r}
# Computing the propensity socre by logistic regression of W on X
p_logistic.fit <- glm(Wcomp ~ as.matrix(sapply(Xcomp, as.numeric)), family = "binomial")
p_logistic <- predict(p_logistic.fit, type = "response")
hist(p_logistic)

dfcomp$p.logistic <- p_logistic

# Let's check if the regression is well calibrated
{plot(smooth.spline(p_logistic, Wcomp, df = 4))
  abline(0,1)}

#Estimate propensity scores with generalized boosted modeling (GBM)
gbm_ps <- function(dataset, psFormula){
  # es: refers to standardized ef-fect size.
  myGBM <- ps(psFormula, data = dataset, n.trees=10000, interaction.depth=4,
              shrinkage=0.01, stop.method=c("es.max"), estimand = "ATT",
              verbose=TRUE)

  #extract estimated propensity scores from object
  gbm_estimations <- myGBM$ps[, 1]
  # notice here we do not need to
  return(gbm_estimations)
}
p_gbm <- gbm_ps(dfcomp, psFormula = psFormula)
dfcomp$propensity.score.gbm <- p_gbm
hist(dfcomp$propensity.score.gbm)
```



```{r ols_regression}
# Direct conditional mean estimation
# Let's define the conditional average treatment effect (CATE)
ate_condmean_ols <- function(dataset) {
  df_centered = data.frame(scale(dataset, center = TRUE, scale = FALSE))
  # Running OLS with full interactions is like running OLS separately on
  # the treated and controls. If the design matrix has been pre-centered,
  # then the W-coefficient corresponds to the ATE.
  lm.interact = lm(Y ~ . * W, data = df_centered)
  tau.hat = as.numeric(coef(lm.interact)["W"])
  se.hat = as.numeric(sqrt(vcovHC(lm.interact)["W", "W"]))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}
tauhat_ols <- ate_condmean_ols(dfcomp)
print(tauhat_ols)
```

```{r}
# Now let's estimate the ATE using Inverse-propensity score weighting
ipw <- function(dataset, p) {
  W <- dataset$W
  Y <- dataset$Y
  G <- ((W - p) * Y) / (p * (1-p))
  tau.hat <- mean(G)
  se.hat <- sqrt(var(G) / (length(G) - 1))
  c(ATE = tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.68 * se.hat)
}

tauhat_logistic_ipw <- ipw(dfcomp, p_logistic)
print(tauhat_logistic_ipw)
```
```{r prop_score_ols}
# Now Weighted OLS on W

prop_score_ols <- function(dataset, p) {
  # Pulling relevant columns
  W <- dataset$W
  Y <- dataset$Y
  # Computing weights
  weights <- (W / p) + ((1 - W) / (1 - p))
  # OLS
  lm.fit <- lm(Y ~ W, data = dataset, weights = weights)
  tau.hat = as.numeric(coef(lm.fit)["W"])
  se.hat = as.numeric(sqrt(vcovHC(lm.fit)["W", "W"]))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}

tauhat_pscore_ols <- prop_score_ols(dfcomp, p_logistic)
print(tauhat_pscore_ols)

```


```{r aipw}
# Now the double robust
aipw_ols <- function(dataset, p) {
  
  ols.fit = lm(Y ~ W * ., data = dataset)
  
  dataset.treatall = dataset
  dataset.treatall$W = 1
  treated_pred = predict(ols.fit, dataset.treatall)
  
  dataset.treatnone = dataset
  dataset.treatnone$W = 0
  control_pred = predict(ols.fit, dataset.treatnone)
  
  actual_pred = predict(ols.fit, dataset)
  
  G <- treated_pred - control_pred +
    ((dataset$W - p) * (dataset$Y - actual_pred)) / (p * (1 - p))
  tau.hat <- mean(G)
  se.hat <- sqrt(var(G) / (length(G) - 1))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}

tauhat_lin_logistic_aipw <- aipw_ols(dfcomp, p_logistic)
print(tauhat_lin_logistic_aipw)
```


Now since we have an estimation of the ATE, let's cluster the dataset using the propensity score as a distance using different clustering methods


Estimating ATT #CAGA DPS ARRUMA
```{r}
get_estimated_treatment <- function(matching_output, zeFormula){
  z.out <- zelig(zeFormula, data = match.data(matching_output, "control"), model = "ls")
  x.out <- setx(z.out, data = match.data(matching_output, "treat"), cond = TRUE)
  s.out <- sim(z.out, x = x.out)
  return (s.out)
}
zeFormula = Ycomp ~ as.matrix(sapply(Xcomp, as.numeric))
psFormula = as.formula(paste("W ~", paste(covariates, collapse = " + ")))
```


Estimating TE (Treatment Effect on Treated) using a matched 1:1 dataset #TO FAZENDO - LUIZ

```{r}
estimate_treatment_effect <- function(dataset, match.matrix){
  treated <- dataset %>% filter(W == 1) %>% select(X, Y)
  control <- dataset %>% filter(W == 0) %>% select(X, Y)
  for (i in treated)
    i$Y_matched <- control %>% filter(X == match.matrix[i$X]) %>% select(Y)
}

treated <- dfcomp %>% filter(W == 1) %>% select(X, Y)
control <- dfcomp %>% filter(W == 0) %>% select(X, Y)

for (i in treated)
    i$Y_matched <- control %>% filter(X == greedymatching$match.matrix[i$X]) %>% select(Y)

```


I) Clustering using Matching - Using the library MatchIt # MUDAR NOME TA ERRADO SSAPORRA
i) Nearest-Neighbor Propensity Score Matching, with Propensity Score estimated with Logistic Regression # Ã‰ 1:1
```{r greedymatching}
# Matching on Nearest Neighbor
neighbor_matching <- function(psFormula, distance, data, method = "nearest", ...) {
  return(matchit(psFormula, distance = distance, data = data, method = method, ...))
}

greedymatching <- neighbor_matching(psFormula, distance = p_logistic, data = dfcomp, method = "nearest")
#greedy_s.out <- get_estimated_treatment(greedymatching, zeFormula)
#summary(greedy_s.out)
```


ii) Nearest-Neighbor Propensity Score Matching, with Propensity Score estimated with XGBoost
```{r}

```


Genetic Matching (1:1)
```{r}
# perform genetic matching based on all the covariates and the propensity score estimated from GMB
calc_geneticMatchin_with_gmb = function(data, psFormula, zeFormula, Ps_scores){
  geneticMatching <- matchit(psFormula, distance = Ps_scores,
                           data = data, method = "genetic",pop.size=1000,
                           fit.func = "pvals",
                           estimand = "ATT", replace = T, ties = T, discard = "both")
}

calc_geneticMatchin_with_gmb(dfcomp, psFormula, zeFormula, Ps_scores = dfcomp$propensity.score.gbm)
```


