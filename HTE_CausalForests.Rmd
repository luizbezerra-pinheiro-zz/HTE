---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 


First of all we import the necessary libraries for the causality study that will follow.

```{r}
library(dplyr)       # Data manipulation (0.8.0.1)

library(fBasics)     # Summary statistics (3042.89)

library(corrplot)    # Correlations (0.84)

library(psych)       # Correlation p-values (1.8.12)

library(grf)         # Generalized random forests (0.10.2)

library(rpart)       # Classification and regression trees, or CART (4.1-13)

library(rpart.plot)  # Plotting trees (3.0.6)

library(treeClust)   # Predicting leaf position for causal trees (1.1-7)

library(car)         # linear hypothesis testing for causal tree (3.0-2)

library(devtools)    # Install packages from github (2.0.1)

library(readr)       # Reading csv files (1.3.1)

library(tidyr)       # Database operations (0.8.3)

library(tibble)      # Modern alternative to data frames (2.1.1)

library(knitr)       # RMarkdown (1.21)

library(kableExtra)  # Prettier RMarkdown (1.0.1)

library(ggplot2)     # general plotting tool (3.1.0)

library(haven)       # read stata files (2.0.0)

library(aod)         # hypothesis testing (1.3.1)

library(evtree)      # evolutionary learning of globally optimal trees (1.0-7)

library(causalTree)

library(causalToolbox)
```

We now read the data set of interest for the pursuit of the study. The present dataset has been treated already, there are no missing values nor NaN in the table.

```{r}
# df <- read.csv("tabela_amelia5.csv", na.strings = c("NA", "ND", "NF", "NR","")  )
#outcome_variable_name <- "Y"
#treatment_variable_name <- "W"
#covariate_names <- c(  "Anticoagulant.therapy","Antiplatelet.therapy", "GCS.init",
#"GCS.motor.init","Pupil.anomaly.ph","Osmotherapy.ph",   "Improv.anomaly.osmo","Cardiac.arrest.ph","SBP.ph","DBP.ph",      "HR.ph",  "SBP.ph.min",   "DBP.ph.min",   "HR.ph.max",    "Cristalloid.volume",   "Colloid.volume","HemoCue.init","Delta.hemoCue","Vasopressor.therapy",  "SpO2.ph.min",  "Medcare.time.ph",  "GCS",  "GCS.motor",    "Pupil.anomaly",    "TCD.PI.max",   "FiO2", "Neurosurgery.day0",    "IGS.II",   "Temperature.min",  "W",    "TBI",  "Osmotherapy",  "IICP", "EVD",  "Decompressive.craniectomy",    "Y",    "AIS.head", "AIS.face", "AIS.external", "ISS",  "Shock.index.ph",   "Delta.shock.index")

df <- read.csv("data_correct.csv") #, na.strings = c("NA", "ND", "NF", "NR","")  )
outcome_variable_name <- "Y"
treatment_variable_name <- "W"
covariate_names <- c("SBP.ph","DBP.ph",
                     "HR.ph","Shock.index.ph","Delta.shock.index",
"Cristalloid.volume","Colloid.volume","HemoCue.init","Delta.hemoCue","SpO2.ph.min",
"Vasopressor.therapy","AIS.external","Cardiac.arrest.ph")

summary(df)
```


```{r}
all_variables_names <- c(outcome_variable_name, treatment_variable_name, covariate_names)
df <- df[, which(names(df) %in% all_variables_names)]

train_fraction <- 0.70  # Use train_fraction % of the dataset to train our models
n <- dim(df)[1]

train_idx <- sample.int(n, replace=F, size=floor(n*train_fraction))
df_train <- df[train_idx,]
df_test <- df[-train_idx,]

```

Detecting heterogeneous treatment effects, i.e., differential effects of an intervention for certain subgroups of the population, can be very valuable in many areas of research. 
Causal trees provide a data-driven approach to partitioning the data into subgroups that differ by the magnitude of their treatment effects. Much like decision trees, which partition the covariate space by finding subgroups with similar *outcomes*, causal trees find subgroups with *similar treatment effects*. Moreover, even though this is an adaptive method, these subgroups do not need to be specified prior to the experiment.

In order to ensure valid estimates of the treatment effect within each subgroup, a sample-splitting approach is used -  **honesty**: a method is honest if it uses one subset of the data to estimate the model parameters, and a different subset to produce estimates given these estimated parameters. In the context of causal trees, honesty is one of the assumptions required to produce unbiased and asymptotically normal estimates of the treatment effect.

#### Step 1: Split the dataset

As we just explained, honesty requires us to separate different subsets of our training data for model selection and prediction.

+ `df_split`: the *splitting sample*, used to build the tree
+ `df_est`: the *estimation sample*, used to compute the average treatment effect in each leaf

```{r}
split_size <- floor(nrow(df_train) * 0.7)
split_idx <- sample(nrow(df_train), replace=FALSE, size=split_size)

# Make the splits
df_split <- df_train[split_idx,]
df_est <- df_train[-split_idx,]
fmla_ct <- paste("factor(Y) ~", paste(covariate_names, collapse = " + "))

ct_unpruned <- honest.causalTree(
  formula=fmla_ct,            # Define the model
  data=df_split,              # Subset used to create tree structure
  est_data=df_est,            # Which data set to use to estimate effects

  treatment=df_split$W,       # Splitting sample treatment variable
  est_treatment=df_est$W,     # Estimation sample treatment variable

  split.Rule="CT",            # Define the splitting option
  cv.option="TOT",            # Cross validation options

  split.Honest=TRUE,          # Use honesty when splitting
  cv.Honest=TRUE,             # Use honesty when performing cross-validation

  minsize=16,                 # Min. number of treatment and control cases in each leaf
  HonestSampleSize=nrow(df_est)) # Num obs used in estimation after building the tree
```


```{r}
rpart.plot(ct_unpruned,type = 4, extra = 0, branch.lty = 3, box.palette = "RdYlGn")


```

```{r}
rpart.plot(ct_unpruned, type = 4, varlen = 0, faclen = 0, fallen.leaves = TRUE)
```



```{r}
rpart.plot(

  x=ct_unpruned,        # Unpruned tree

  type=3,             # Draw separate split labels for the left and right directions

  fallen=TRUE,        # Position the leaf nodes at the bottom of the graph

  leaf.round=1,       # Rounding of the corners of the leaf node boxes

  extra=100,          # Display the percentage of observations in the node

  branch=.1,          # Shape of the branch lines

  box.palette="RdBu") # Palette for coloring the node


```



## Estimating the Treatment with Causal Forests

We start by training a pilot random forest on all the features, and then train a second forest on only those features that saw a reasonable number of splits in the first forest. This enables the second forest to make better splits and use most useful features.

```{r}
Y <- df[, which(names(df) %in% outcome_variable_name)]
W <- df[, which(names(df) %in% treatment_variable_name)]
x_covariate_names <- c("SBP.ph","DBP.ph",
                     "HR.ph","Shock.index.ph","Delta.shock.index",
"Cristalloid.volume","Colloid.volume","HemoCue.init","Delta.hemoCue","SpO2.ph.min",
"Vasopressor.therapy","AIS.external","Cardiac.arrest.ph")

X <- df[, which(names(df) %in% x_covariate_names)]
X <- data.frame(lapply(X, function(x) as.numeric(as.character(x))))

y_forest <- regression_forest(X, Y) #Forest for the expected outcome
y_hat <- predict(y_forest)$predictions

w_forest <- regression_forest(X, W) #forest for the propensity score
w_hat <- predict(w_forest)$predictions

cf_raw <- causal_forest(X, Y, W,
                        Y.hat = y_hat, W.hat = w_hat)

varimp <- variable_importance(cf_raw)

selected_id <- which(varimp > mean(varimp)) #Here we select the variables that were best fitted in the Forest
  
cf <- causal_forest(X[,selected_id], Y, W,
                    Y.hat = y_hat, W.hat = w_hat,
                    tune.parameters = TRUE)
tau_hat <- predict(cf)$predictions

hist(tau_hat) #Histogram of out-of-bag CATE estimates from a causal forest trained above
```


We call here the average treatment function (from the grf package).

```{r}
ATE <- average_treatment_effect(cf)
paste( " 9 5 % CI for the ATE : " , round ( ATE [1] , 3 ) ,
" +/ - " , round ( qnorm (0.975) * ATE [2] , 3 ))
```


In order to test the heterogeneity learned we can compare if the out-of-bag CATE estimates (tau_hat) are above or below the median CATE estimates, and then we estimate the ATE in these two subgroups separately.

```{r}
# Compare regions with high and low estimated CATEs

high_effect <- tau_hat > median(tau_hat) #Selection of those above the median

#Make the predictions for each subgroup
ate_high <- average_treatment_effect(cf, subset = high_effect) 
ate_low <- average_treatment_effect(cf, subset = !high_effect)

paste( " 9 5 % CI for the ATE : " , round ( ate_high[1] - ate_low[1] , 3 ) ,
" +/ - " , round ( qnorm (0.975) * sqrt(ate_high[2]^2 + ate_low[2]^2) , 3 ))

```

Another possible approach for testing heterogeneity is by making the best linear predictor comparison. In this case we search to fit the CATE as a linear function of the out-of-bag causal forest estimates. We make the regression wrt
$C_{i} = \overline{\tau}(W_i - \hat{e}^{(-i)}(X_i))$ and $ D_i = (\hat{\tau}^{(-i)}(X_i) - \overline{\tau})(W_i - \hat{e}^{(-i)}(X_i))$.

We regress $Y_i - \hat{m}^{(-i)}(X_i)$ against $C_i$ and $D_i$. The coefficient of $D_i$ can be interpreted as a measure of the quality of the estimates of treatment heterogeneity. Formally: the p-value for the second coefficient  will be used to test the hypothesis that the causal forest succeeded in finding heterogeneity.
```{r}
test_calibration(cf)
```

The causal forest has slightly succeeded in accurately estimating treatment heterogeneity. 
We see that the estimator upon this dataset reveals no strong causality effect with the Tranexemic Acid administration and the expected outcome.
